{"hash":"65f77092d4c7d3e9a7fd369dc1b06b4f37ffc394","data":{"modelThreatsPost":{"title":"PICKLE-MALCODE","path":"/model-threats/pickle-malcode/","content":"<h2 id=\"overview\"><a href=\"#overview\" aria-hidden=\"true\" tabindex=\"-1\">Overview</a></h2>\n<p>A Pickle-based model contains serialized <a href=\"https://docs.python.org/3/library/pickle.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Pickle</a> data which may cause <strong>execution of malicious Python code</strong> when the model is loaded.</p>\n<p>Many ML model formats such as PyTorch, JobLib, NumPy and more, use Python's Pickle serialization format as part of their internal storage.</p>\n<p>The Pickle format is well-known to be a <strong>dangerous</strong> serialization format, since in addition to serialized data, it may contain serialized code which will be automatically executed when the Pickled/Serialized file is loaded.</p>\n<p><img src=\"/img/pickle_deserialization.png\"></p>\n<h2 id=\"time-of-infection\"><a href=\"#time-of-infection\" aria-hidden=\"true\" tabindex=\"-1\">Time of Infection</a></h2>\n<p><strong>[v] Model Load</strong></p>\n<p>[] Model Query</p>\n<p>[] Other</p>\n<h2 id=\"legitimacy-of-embedded-pickle-code\"><a href=\"#legitimacy-of-embedded-pickle-code\" aria-hidden=\"true\" tabindex=\"-1\">Legitimacy of embedded Pickle code</a></h2>\n<p>ML model authors may often bundle their model with \"setup code\" - code that is meant to run on the machine that uses the model, before the actual model data is used. This code can, for example, set up needed environment variables, perform optimization based on the local machine parameters or download extra needed dependencies.</p>\n<p>However - with relation to Pickle-based formats, <strong>it is extremely rare that ML model authors would choose to inject the legitimate model setup code into the model's Pickle data.</strong></p>\n<p>Rather - this code is bundled outside of the Pickle data, and is usually executed either -</p>\n<ol>\n<li>Manually, by the model user that read the model's usage guide</li>\n<li>Automatically, by some other code execution capability provided by the loading library</li>\n</ol>\n<p>Therefore - auto-executing code that is found in the Model's Pickle data has a very high chance of being malicious.</p>\n<h2 id=\"evidence-extraction-and-false-positive-elimination\"><a href=\"#evidence-extraction-and-false-positive-elimination\" aria-hidden=\"true\" tabindex=\"-1\">Evidence Extraction and False Positive Elimination</a></h2>\n<p>To safely determine if the suspected model contains malicious Pickled code -</p>\n<ol>\n<li>Extract the Pickle data from the ML model (this step differs for each ML model format)</li>\n<li>\n<p>Decompile the Pickle data, ex. using <a href=\"https://github.com/trailofbits/fickling\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Fickling</a> and assuming data was extracted to <code>data.pkl</code> -</p>\n<pre><code class=\"language-python\">import ast\nfrom fickling.fickle import Pickled\n\npickledata = open(\"data.pkl\", \"rb\").read()\nfickled_object = Pickled.load(pickledata)\nprint(ast.dump(fickled_object.ast, indent=4))\n</code></pre>\n</li>\n<li>Examine the decompiled Python code to determine if it contains any malicious instructions</li>\n</ol>\n<p>JFrog conducts extraction, decompilation and detailed analysis on each Pickle-based model in order to determine whether any malicious code is present.</p>\n<h2 id=\"additional-information\"><a href=\"#additional-information\" aria-hidden=\"true\" tabindex=\"-1\">Additional Information</a></h2>\n<ul>\n<li><a href=\"https://blog.trailofbits.com/2024/06/11/exploiting-ml-models-with-pickle-file-attacks-part-1/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://blog.trailofbits.com/2024/06/11/exploiting-ml-models-with-pickle-file-attacks-part-1/</a></li>\n</ul>\n","description":"Pickle-based model with embedded malicious code"}},"context":{}}