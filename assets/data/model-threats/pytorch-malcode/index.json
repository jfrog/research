{"hash":"b91ce5bd5d6cb0faf79d9409808437f60abad9f7","data":{"modelThreatsPost":{"title":"PYTORCH-MALCODE","path":"/model-threats/pytorch-malcode/","content":"<h2 id=\"overview\"><a href=\"#overview\" aria-hidden=\"true\" tabindex=\"-1\">Overview</a></h2>\n<p>A PyTorch model contains serialized <a href=\"https://docs.python.org/3/library/pickle.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Pickle</a> data which may cause <strong>execution of malicious Python code</strong> when the model is loaded. </p>\n<p>The PyTorch model format internally uses Python's Pickle data serialization format. </p>\n<p><img src=\"/img/pytorch_format.png\"></p>\n<p>The Pickle format is well-known to be a <strong>dangerous</strong> serialization format, since in addition to serialized data, it may contain serialized code which will be automatically executed when the Pickled/Serialized file is loaded.</p>\n<p><img src=\"/img/pickle_deserialization.png\"></p>\n<h2 id=\"time-of-infection\"><a href=\"#time-of-infection\" aria-hidden=\"true\" tabindex=\"-1\">Time of Infection</a></h2>\n<p><strong>[v] Model Load</strong></p>\n<p>[] Model Query</p>\n<p>[] Other</p>\n<h2 id=\"legitimacy-of-embedded-pickle-code\"><a href=\"#legitimacy-of-embedded-pickle-code\" aria-hidden=\"true\" tabindex=\"-1\">Legitimacy of embedded Pickle code</a></h2>\n<p>ML model authors may often bundle their model with \"setup code\" - code that is meant to run on the machine that uses the model, before the actual model data is used. This code can, for example, set up needed environment variables, perform optimization based on the local machine parameters or download extra needed dependencies.</p>\n<p>However - with relation to Pickle-based formats, <strong>it is extremely rare that ML model authors would choose to inject the legitimate model setup code into the model's Pickle data.</strong></p>\n<p>Rather - this code is bundled outside of the Pickle data, and is usually executed either -</p>\n<ol>\n<li>Manually, by the model user that read the model's usage guide</li>\n<li>Automatically, by some other code execution capability provided by the loading library</li>\n</ol>\n<p>Therefore - auto-executing code that is found in the Model's Pickle data has a very high chance of being malicious.</p>\n<h2 id=\"evidence-extraction-and-false-positive-elimination\"><a href=\"#evidence-extraction-and-false-positive-elimination\" aria-hidden=\"true\" tabindex=\"-1\">Evidence Extraction and False Positive Elimination</a></h2>\n<p>To safely determine if the suspected model contains malicious Pickled code -</p>\n<ol>\n<li>Extract the suspected PyTorch model as a ZIP archive, to retrieve the \"data.pkl\" Pickle file</li>\n<li>\n<p>Decompile data.pkl, ex. using <a href=\"https://github.com/trailofbits/fickling\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Fickling</a> -</p>\n<pre><code class=\"language-python\">import ast\nfrom fickling.fickle import Pickled\n\npickledata = open(\"data.pkl\", \"rb\").read()\nfickled_object = Pickled.load(pickledata)\nprint(ast.dump(fickled_object.ast, indent=4))\n</code></pre>\n</li>\n<li>Examine the decompiled Python code to determine if it contains any malicious instructions</li>\n</ol>\n<p>JFrog conducts extraction, decompilation and detailed analysis on each PyTorch model in order to determine whether any malicious code is present.</p>\n<h2 id=\"additional-information\"><a href=\"#additional-information\" aria-hidden=\"true\" tabindex=\"-1\">Additional Information</a></h2>\n<ul>\n<li><a href=\"https://discuss.pytorch.org/t/securely-serializing-loading-untrusted-pytorch-models/119744\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://discuss.pytorch.org/t/securely-serializing-loading-untrusted-pytorch-models/119744</a></li>\n</ul>\n","description":"PyTorch model with embedded malicious code"}},"context":{}}